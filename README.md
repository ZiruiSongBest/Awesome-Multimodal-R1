# Awesome-Multimodal-R1
Paper list of Multimodal R1(GRPO) model
## Multimodal R1 (GRPO) Paper List

This repository contains a curated list of papers related to Multimodal R1 (GRPO) models.

| Paper | Date | Input Modality | Code | Brief |
|-------|------|----------------|------|-------|
| [R1-Omni: Explainable Omni-Multimodal Emotion Recognition with Reinforcement Learning](https://arxiv.org/pdf/2503.05379) | 2025-03 | Audio + Video | [Code](https://github.com/HumanMLLM/R1-Omni) [![](https://img.shields.io/github/stars/HumanMLLM/R1-Omni)](https://github.com/HumanMLLM/R1-Omni) | Focus on emotion recognition, a task where both visual and audio modalities |
| [R1-Zero's "Aha Moment" in Visual Reasoning on a 2B Non-SFT Model](https://arxiv.org/abs/2503.05379) | 2025-03 | Image | [Code](https://github.com/turningpoint-ai/VisualThinker-R1-Zero) [![](https://img.shields.io/github/stars/turningpoint-ai/VisualThinker-R1-Zero)](https://github.com/turningpoint-ai/VisualThinker-R1-Zero) | Starting with Qwen2-VL-2B and applying reinforcement learning directly on the SAT dataset|
| [Paper Title 3](https://arxiv.org/abs/xxxx.xxxxx) | YYYY-MM |  | [Code](https://github.com/username/repo) [![](https://img.shields.io/github/stars/username/repo)](https://github.com/username/repo) | Brief description of the paper |

## How to Contribute

1. Fork the repository
2. Add your paper to the list following the format above
3. Submit a pull request

## Citation

If you find this repository helpful, please consider citing:

